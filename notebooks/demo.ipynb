{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from transformers import AutoProcessor, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "from datasets import load_from_disk\n",
    "sys.path.append(str(Path.cwd().parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.dirutils import get_data_dir, get_models_dir\n",
    "from src.models.multiclassification.predict_model import ViTForMultiClassificationPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224-in21k were not used when initializing ViTModel: ['pooler.dense.weight', 'pooler.dense.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "multiclassification_model = ViTForMultiClassificationPredictor(\n",
    "    get_models_dir() / \"multiclassification\" / \"full\" / \"model-20230513_121917-35.pt\",\n",
    "    DEVICE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-2.0949, -2.0840, -2.4154, -5.1525, -5.7846], device='cuda:0',\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multiclassification_model.model.log_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_from_disk(get_data_dir() / \"processed\" / \"captioning_dataset_augmented_processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def capitalize_artist(s):\n",
    "    return \" \".join([word.capitalize() for word in s.split(\"-\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclassification_prediction_to_prompt(prediction):\n",
    "    prompt = \"\"\n",
    "    multiclass_preds = prediction[0]\n",
    "    multilabel_preds = prediction[1]\n",
    "    for feature, pred in multiclass_preds.items():\n",
    "        if feature == \"artist\":\n",
    "            if pred[0] == \"other\":\n",
    "                continue\n",
    "            else:\n",
    "                # remove dash and capitalize first letter of each word\n",
    "                pred = capitalize_artist(pred[0])\n",
    "                prompt += f\"{pred} \"\n",
    "        else:\n",
    "            prompt += f\"{pred[0].capitalize()} \"\n",
    "    # for feature, pred in multilabel_preds.items():\n",
    "    #     for label in pred:\n",
    "    #         prompt += f\"{label} \"\n",
    "    return prompt.strip()\n",
    "\n",
    "def multiclassification_prediction_to_caption(prediction):\n",
    "    caption = \"\"\n",
    "    multiclass_preds = prediction[0]\n",
    "    multilabel_preds = prediction[1]\n",
    "\n",
    "    caption += f\"The artwork is a {multiclass_preds['genre'][0].capitalize()}\"\n",
    "    caption += f\", in the style of {multiclass_preds['style'][0].capitalize()}.\"\n",
    "\n",
    "    if multiclass_preds[\"artist\"][0] != \"other\":\n",
    "        artist_pred = capitalize_artist(multiclass_preds[\"artist\"][0])\n",
    "        caption += f\" It could be by {artist_pred}.\"\n",
    "    \n",
    "    if multilabel_preds[\"media\"]:\n",
    "        caption += f\" The used media are {', '.join(multilabel_preds['media'])}.\"\n",
    "\n",
    "    if multilabel_preds[\"tags\"]:\n",
    "        caption += f\" It is about {', '.join(multilabel_preds['tags'])}.\"\n",
    "\n",
    "    return caption.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GitForCausalLM(\n",
       "  (git): GitModel(\n",
       "    (embeddings): GitEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(1024, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (image_encoder): GitVisionModel(\n",
       "      (vision_model): GitVisionTransformer(\n",
       "        (embeddings): GitVisionEmbeddings(\n",
       "          (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
       "          (position_embedding): Embedding(197, 768)\n",
       "        )\n",
       "        (pre_layrnorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (encoder): GitVisionEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x GitVisionEncoderLayer(\n",
       "              (self_attn): GitVisionAttention(\n",
       "                (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (mlp): GitVisionMLP(\n",
       "                (activation_fn): QuickGELUActivation()\n",
       "                (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              )\n",
       "              (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (encoder): GitEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-5): 6 x GitLayer(\n",
       "          (attention): GitAttention(\n",
       "            (self): GitSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): GitSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): GitIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): GitOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (visual_projection): GitProjection(\n",
       "      (visual_projection): Sequential(\n",
       "        (0): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output): Linear(in_features=768, out_features=30522, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL_NAME = \"microsoft/git-base\"\n",
    "PROCESSOR = AutoProcessor.from_pretrained(MODEL_NAME)\n",
    "MODEL = AutoModelForCausalLM.from_pretrained(MODEL_NAME, torch_dtype=torch.float16)\n",
    "OUTPUT_DIR = \"microsoft_git-base_artgraph\"\n",
    "\n",
    "checkpoint = torch.load(get_models_dir() / \"captioning\" / \"microsoft-git-base-good-samples\" / \"2.pt\")\n",
    "MODEL.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "MODEL.to(DEVICE)\n",
    "MODEL.train(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://850a3165d124aaa807.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://850a3165d124aaa807.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a gradio interface with image input and text output\n",
    "import gradio as gr\n",
    "\n",
    "def captioning_pipeline(image):\n",
    "    prediction = multiclassification_model.predict(image)\n",
    "    caption = multiclassification_prediction_to_caption(prediction)\n",
    "    pixel_values = PROCESSOR(images=image, return_tensors=\"pt\").pixel_values.to(DEVICE, torch.float16)\n",
    "    generated_ids = MODEL.generate(pixel_values=pixel_values, min_length=12, max_length=50, num_beams=4, no_repeat_ngram_size=2, do_sample=False, temperature=1.5)\n",
    "    generated_caption = PROCESSOR.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    caption += f\" The artwork depicts {generated_caption}.\"\n",
    "    return caption\n",
    "\n",
    "def prompt_pipeline(prompt):\n",
    "    prediction = multiclassification_model.predict(prompt)\n",
    "    prompt = multiclassification_prediction_to_prompt(prediction)\n",
    "    return prompt\n",
    "\n",
    "image = gr.components.Image()\n",
    "caption = gr.components.Textbox()\n",
    "\n",
    "gr.Interface(\n",
    "    captioning_pipeline,\n",
    "    image,\n",
    "    caption,\n",
    "    title=\"Artwork Captioning\",\n",
    "    description=\"Generate a caption for an artwork.\",\n",
    "    examples=[\n",
    "        [\"https://uploads2.wikiart.org/images/ivan-aivazovsky/moonlit-night-beside-the-sea-1847.jpg!Large.jpg\"],\n",
    "        [\"https://uploads4.wikiart.org/images/georges-seurat/sunday-afternoon-on-the-island-of-la-grande-jatte-1886.jpg!Large.jpg\"],\n",
    "        [\"https://uploads2.wikiart.org/images/edvard-munch/the-scream-1893(2).jpg!Large.jpg\"]\n",
    "    ]\n",
    "    ).launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
