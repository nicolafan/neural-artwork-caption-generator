{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PosixPath('/home/nico/Thesis/neural-artwork-caption-generator')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PROJECT_DIR = Path(os.getcwd()).resolve().parent\n",
    "PROJECT_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(str(PROJECT_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nico/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from src.models.multiclassification.model import ViTForMultiClassification\n",
    "from src.models.multiclassification.data import get_multiclassification_dicts\n",
    "from src.utils.dirutils import get_data_dir\n",
    "from datasets import Dataset, load_from_disk\n",
    "from transformers import ViTImageProcessor\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at google/vit-base-patch16-224 were not used when initializing ViTModel: ['classifier.weight', 'classifier.bias']\n",
      "- This IS expected if you are initializing ViTModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ViTModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "multiclass_classifications, multilabel_classifications = get_multiclassification_dicts()\n",
    "multiclass_features, multilabel_features = list(multiclass_classifications.keys()), list(multilabel_classifications.keys())\n",
    "all_features = multiclass_features + multilabel_features\n",
    "model = ViTForMultiClassification(multiclass_classifications, multilabel_classifications)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "dataset: Dataset = load_from_disk(\n",
    "    get_data_dir() / \"processed\" / \"multiclassification_dataset\"\n",
    ")\n",
    "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(examples):\n",
    "    examples[\"pixel_values\"] = processor(examples[\"image\"], return_tensors=\"pt\").pixel_values\n",
    "    new_examples = {}\n",
    "    for feature in multiclass_features:\n",
    "        new_examples[feature] = torch.tensor(examples[feature], dtype=torch.long, device=device)\n",
    "    for feature in multilabel_features:\n",
    "        new_examples[feature] = torch.tensor(examples[feature], dtype=torch.float, device=device)\n",
    "    new_examples[\"pixel_values\"] = examples[\"pixel_values\"].to(device)\n",
    "    return new_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.with_transform(process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(dataset[\"train\"].select(range(1000)), batch_size=8)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset[\"validation\"].select(range(1000)), batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom loss for multilabel classification, ignore elements that have 0 labels\n",
    "def binary_cross_entropy_with_logits_ignore_no_labels(preds, targets):\n",
    "    zero_labels = torch.sum(targets, dim=-1) == 0\n",
    "    targets = targets.float()\n",
    "\n",
    "    # Apply BCEWithLogitsLoss only to non-zero labels\n",
    "    loss = F.binary_cross_entropy_with_logits(preds, targets, reduction=\"none\")\n",
    "    loss = torch.where(zero_labels.unsqueeze(-1), torch.zeros_like(loss), loss)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def losses_fn(outputs, targets):\n",
    "    losses = []\n",
    "    for feature in multiclass_features:\n",
    "        loss = F.cross_entropy(outputs[feature], targets[feature].squeeze(), ignore_index=-1)\n",
    "        losses.append(loss)\n",
    "    for feature in multilabel_features:\n",
    "        loss = binary_cross_entropy_with_logits_ignore_no_labels(outputs[feature], targets[feature])\n",
    "        losses.append(loss)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_losses(model, losses):\n",
    "    final_losses = []\n",
    "    if model.log_vars is not None:\n",
    "        for i, loss in enumerate(losses):\n",
    "            final_losses.append(torch.exp(-model.log_vars[i]) * loss + model.log_vars[i]/2)\n",
    "    else:\n",
    "        final_losses = losses\n",
    "    return sum(final_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch():\n",
    "    epoch_loss = 0.\n",
    "    epoch_label_losses = [0.] * len(all_features)\n",
    "\n",
    "    n_batches = len(train_loader)\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for batch in tqdm(train_loader):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, targets = batch[\"pixel_values\"], {k: batch[k] for k in batch if k != \"pixel_values\"}\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        losses = losses_fn(outputs, targets)\n",
    "        loss = join_losses(model, losses)\n",
    "        \n",
    "        # Update our running loss tally\n",
    "        for i, _ in enumerate(all_features):\n",
    "            epoch_label_losses[i] += losses[i].item()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "        # Compute the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return epoch_loss / n_batches, [loss / n_batches for loss in epoch_label_losses]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_batch_arrays(all_data):\n",
    "    all_data_concat = {}\n",
    "    for d in all_data:\n",
    "        for key in d:\n",
    "            if key not in all_data_concat:\n",
    "                all_data_concat[key] = d[key]\n",
    "            else:\n",
    "                all_data_concat[key] = np.concatenate((all_data_concat[key], d[key]), axis=0)\n",
    "    return all_data_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval():\n",
    "    # Validate\n",
    "    running_vloss = 0.\n",
    "    running_label_vlosses = [0.] * len(all_features)\n",
    "    all_voutputs = []\n",
    "    all_vtargets = []\n",
    "    \n",
    "    for vbatch in tqdm(validation_loader):\n",
    "        # Compute batch outputs\n",
    "        vinputs, vtargets = vbatch[\"pixel_values\"], {k: vbatch[k] for k in vbatch if k != \"pixel_values\"}\n",
    "        voutputs = model(vinputs)\n",
    "\n",
    "        # Compute batch losses\n",
    "        vlosses = losses_fn(voutputs, vtargets)\n",
    "        vloss = join_losses(model, vlosses)\n",
    "        for i, _ in enumerate(all_features):\n",
    "            running_label_vlosses[i] += vlosses[i].item()\n",
    "        running_vloss += vloss.item()\n",
    "\n",
    "        # Save predictions and targets for later\n",
    "        all_voutputs.append(\n",
    "            {k: v.detach().cpu().numpy() for k, v in voutputs.items()}\n",
    "        )\n",
    "        all_vtargets.append(\n",
    "            {k: v.detach().cpu().numpy() for k, v in vtargets.items()}\n",
    "        )\n",
    "\n",
    "    avg_vloss = running_vloss / len(validation_loader)\n",
    "    running_label_vlosses = [loss / len(validation_loader) for loss in running_label_vlosses]\n",
    "    all_voutputs = concatenate_batch_arrays(all_voutputs)\n",
    "    all_vtargets = concatenate_batch_arrays(all_vtargets)\n",
    "    return avg_vloss, running_label_vlosses, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 5/125 [00:04<01:50,  1.08it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39m# Make sure gradient tracking is on, and do a pass over the data\u001b[39;00m\n\u001b[1;32m     14\u001b[0m model\u001b[39m.\u001b[39mtrain(\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m train_loss, train_label_losses \u001b[39m=\u001b[39m train_one_epoch()\n\u001b[1;32m     16\u001b[0m \u001b[39mfor\u001b[39;00m i, label \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(all_features):\n\u001b[1;32m     17\u001b[0m     writer\u001b[39m.\u001b[39madd_scalar(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mtrain/\u001b[39m\u001b[39m{\u001b[39;00mlabel\u001b[39m}\u001b[39;00m\u001b[39m_loss\u001b[39m\u001b[39m\"\u001b[39m, train_label_losses[i], epoch_number \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m n_batches \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(train_loader)\n\u001b[1;32m      7\u001b[0m \u001b[39m# Here, we use enumerate(training_loader) instead of\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# iter(training_loader) so that we can track the batch\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[39m# index and do some intra-epoch reporting\u001b[39;00m\n\u001b[0;32m---> 10\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     11\u001b[0m     \u001b[39m# Every data instance is an input + label pair\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     inputs, targets \u001b[39m=\u001b[39m batch[\u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m], {k: batch[k] \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m batch \u001b[39mif\u001b[39;00m k \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpixel_values\u001b[39m\u001b[39m\"\u001b[39m}\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Zero your gradients for every batch!\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/tqdm/std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    635\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset, \u001b[39m\"\u001b[39m\u001b[39m__getitems__\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset\u001b[39m.\u001b[39;49m__getitems__(possibly_batched_index)\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2713\u001b[0m, in \u001b[0;36mDataset.__getitems__\u001b[0;34m(self, keys)\u001b[0m\n\u001b[1;32m   2711\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitems__\u001b[39m(\u001b[39mself\u001b[39m, keys: List) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List:\n\u001b[1;32m   2712\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to get a batch using a list of integers indices.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2713\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m\u001b[39m__getitem__\u001b[39;49m(keys)\n\u001b[1;32m   2714\u001b[0m     n_examples \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch[\u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(batch))])\n\u001b[1;32m   2715\u001b[0m     \u001b[39mreturn\u001b[39;00m [{col: array[i] \u001b[39mfor\u001b[39;00m col, array \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems()} \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_examples)]\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2709\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2707\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, key):  \u001b[39m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2708\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2709\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_getitem(key)\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/arrow_dataset.py:2694\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, **kwargs)\u001b[0m\n\u001b[1;32m   2692\u001b[0m formatter \u001b[39m=\u001b[39m get_formatter(format_type, features\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info\u001b[39m.\u001b[39mfeatures, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mformat_kwargs)\n\u001b[1;32m   2693\u001b[0m pa_subtable \u001b[39m=\u001b[39m query_table(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data, key, indices\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m-> 2694\u001b[0m formatted_output \u001b[39m=\u001b[39m format_table(\n\u001b[1;32m   2695\u001b[0m     pa_subtable, key, formatter\u001b[39m=\u001b[39;49mformatter, format_columns\u001b[39m=\u001b[39;49mformat_columns, output_all_columns\u001b[39m=\u001b[39;49moutput_all_columns\n\u001b[1;32m   2696\u001b[0m )\n\u001b[1;32m   2697\u001b[0m \u001b[39mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:634\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    632\u001b[0m python_formatter \u001b[39m=\u001b[39m PythonFormatter(features\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m format_columns \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 634\u001b[0m     \u001b[39mreturn\u001b[39;00m formatter(pa_table, query_type\u001b[39m=\u001b[39;49mquery_type)\n\u001b[1;32m    635\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mcolumn\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    636\u001b[0m     \u001b[39mif\u001b[39;00m key \u001b[39min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:410\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mformat_column(pa_table)\n\u001b[1;32m    409\u001b[0m \u001b[39melif\u001b[39;00m query_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mbatch\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mformat_batch(pa_table)\n",
      "File \u001b[0;32m~/Thesis/neural-artwork-caption-generator/.venv/lib/python3.10/site-packages/datasets/formatting/formatting.py:520\u001b[0m, in \u001b[0;36mCustomFormatter.format_batch\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    518\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_arrow_extractor()\u001b[39m.\u001b[39mextract_batch(pa_table)\n\u001b[1;32m    519\u001b[0m batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpython_features_decoder\u001b[39m.\u001b[39mdecode_batch(batch)\n\u001b[0;32m--> 520\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(batch)\n",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m, in \u001b[0;36mprocess\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m      3\u001b[0m new_examples \u001b[39m=\u001b[39m {}\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m multiclass_features:\n\u001b[0;32m----> 5\u001b[0m     new_examples[feature] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(examples[feature], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mlong, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[1;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m multilabel_features:\n\u001b[1;32m      7\u001b[0m     new_examples[feature] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(examples[feature], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat, device\u001b[39m=\u001b[39mdevice)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter('runs/fashion_trainer_{}'.format(timestamp))\n",
    "epoch_number = 0\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('EPOCH {}:'.format(epoch_number + 1))\n",
    "\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    train_loss, train_label_losses = train_one_epoch()\n",
    "    for i, label in enumerate(all_features):\n",
    "        writer.add_scalar(f\"train/{label}_loss\", train_label_losses[i], epoch_number + 1)\n",
    "    writer.add_scalar(\"train/loss\", train_loss, epoch_number + 1)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(torch.cuda.memory_summary())\n",
    "\n",
    "    # We don't need gradients on to do reporting\n",
    "    model.train(False)\n",
    "    avg_vloss, running_label_vlosses, metrics = eval() \n",
    "    print('LOSS train {} valid {}'.format(train_loss, avg_vloss))\n",
    "\n",
    "    # Log valid losses\n",
    "    for i, label in enumerate(all_features):\n",
    "        writer.add_scalar(f\"valid/{label}_loss\", running_label_vlosses[i] / len(validation_loader), epoch_number + 1)\n",
    "    writer.add_scalar(\"valid/loss\", avg_vloss, epoch_number + 1)\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('train_vs_valid_loss',\n",
    "                    {'train': train_loss, 'valid': avg_vloss},\n",
    "                    epoch_number + 1)\n",
    "    for k, v in metrics.items():\n",
    "        writer.add_scalar(f'valid/{k}', v, epoch_number + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    model_path = f\"model-{timestamp}-{epoch_number + 1}.pt\"\n",
    "    torch.save({\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict()}, model_path)\n",
    "\n",
    "    epoch_number += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
